{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "hyper-opt-cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rniazbgwSHdR",
        "outputId": "26ed6727-78ac-4b25-dc4c-d286e253f4ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install scikit-optimize"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.6/dist-packages (0.8.1)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (20.4.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8Hra0WRmh0",
        "outputId": "445eb941-3d68-4cf4-aa54-7708b1438b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch Bayesian Optimization Example')\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "                    help='input batch size for training (default: 128)')\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
        "                    help='input batch size for testing (default: 1000)')\n",
        "parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
        "                    help='number of epochs to train (default: 14)')\n",
        "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
        "                    help='learning rate (default: 0.1)')\n",
        "parser.add_argument('--optim', type=str, default='sgd', choices=['sgd', 'adamw'], metavar='M',\n",
        "                    help='Optimization algorithm (default: sgd)')\n",
        "parser.add_argument('--criterion', type=str, default='cross_entropy', choices=['cross_entropy',], metavar='M',\n",
        "                    help='Loss function (default: cross_entropy)')\n",
        "parser.add_argument('--gamma', type=float, default=0.1, metavar='M',\n",
        "                    help='Learning rate step gamma (default: 0.1)')\n",
        "parser.add_argument('--weight-decay', type=float, default=1e-4, metavar='WD',\n",
        "                    help='L2 weight decay coefficeint (default: 1e-4)')\n",
        "parser.add_argument('--model', type=str, default='resnet20', choices=['resnet20', 'resnet32', 'resnet44'], metavar='M',\n",
        "                    help='Resnet model (default: resnet20)')\n",
        "\n",
        "args, _ = parser.parse_known_args()\n",
        "print(args)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=128, criterion='cross_entropy', epochs=50, gamma=0.1, lr=0.1, model='resnet20', optim='sgd', test_batch_size=1000, weight_decay=0.0001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy6Bjm3kRmh3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCbxM6P6Rmh6"
      },
      "source": [
        "import os\n",
        "import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "transform_train = transforms.Compose([transforms.RandomCrop(32, padding=4), \n",
        "                                      transforms.RandomHorizontalFlip(), \n",
        "                                      transforms.ToTensor(), \n",
        "                                      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "transform_eval = transforms.Compose([transforms.ToTensor(), \n",
        "                                     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "def train(model):\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./data', train=True, download=True, transform=transform_train),\n",
        "                                               batch_size=args.batch_size,\n",
        "                                               num_workers=os.cpu_count(),\n",
        "                                               pin_memory=True,\n",
        "                                               shuffle=True)\n",
        "\n",
        "    criterion = {'cross_entropy': nn.CrossEntropyLoss(), \n",
        "                }[args.criterion]\n",
        "    optimizer = {'sgd': torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.weight_decay),\n",
        "                 'adamw': torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "                }[args.optim]\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[args.epochs/2, args.epochs*3/4], gamma=args.gamma)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in tqdm.trange(1, args.epochs + 1):\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "def evaluate(model):\n",
        "    \"\"\"return test accuracy\"\"\"\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10('./data', train=False, download=True, transform=transform_eval), \n",
        "                                               batch_size=args.batch_size,\n",
        "                                               num_workers=os.cpu_count(),\n",
        "                                               pin_memory=True,\n",
        "                                               shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    return test_acc"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUq4V3tTRmh8"
      },
      "source": [
        "## Run hyperparameter search\n",
        "### Search space types\n",
        "- `skopt.space.Real`\n",
        "- `skopt.space.Integer`\n",
        "- `skopt.space.Categorical`\n",
        "\n",
        "Use log-uniform prior if the search space is very wide: `prior='log-uniform'`\n",
        "\n",
        "### Reference for `skopt.gp_minimize()`\n",
        "- https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html\n",
        "- https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html\n",
        "\n",
        "### Advanced (parallelized query)\n",
        "- https://scikit-optimize.github.io/stable/auto_examples/parallel-optimization.html\n",
        "- https://github.com/scikit-optimize/scikit-optimize/issues/737"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PotFgTOWRmh9",
        "outputId": "814a2976-5bda-4d9f-ba9b-28e9f6b8adb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "%%time\n",
        "import skopt\n",
        "\n",
        "dimensions = [skopt.space.Real(low=1e-2, high=1e0, prior='log-uniform', name='lr'),\n",
        "              skopt.space.Real(low=1e-6, high=1e-2, prior='log-uniform', name='weight_decay'),\n",
        "              skopt.space.Categorical(categories=['sgd', 'adamw'], name='optim')]\n",
        "\n",
        "@skopt.utils.use_named_args(dimensions=dimensions)\n",
        "def meta_objective(**kwargs): # numeric args are passed as np.array\n",
        "    args.lr = float(kwargs['lr'])\n",
        "    args.weight_decay = float(kwargs['weight_decay'])\n",
        "    args.optim = kwargs['optim']\n",
        "    \n",
        "    model = resnet20().to(device)\n",
        "    train(model)\n",
        "    test_acc = evaluate(model)\n",
        "    return -test_acc\n",
        "\n",
        "search_result = skopt.gp_minimize(func=meta_objective,\n",
        "                            dimensions=dimensions,\n",
        "                            acq_func='EI', # Expected Improvement.\n",
        "                            n_calls=15,\n",
        "                            n_initial_points=5,\n",
        "                            initial_point_generator='random',\n",
        "                            verbose=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration No: 1 started. Evaluating function at random point.\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/50 [00:16<13:11, 16.15s/it]\u001b[A\n",
            "  4%|▍         | 2/50 [00:32<12:56, 16.18s/it]\u001b[A\n",
            "  6%|▌         | 3/50 [00:48<12:44, 16.26s/it]\u001b[A\n",
            "  8%|▊         | 4/50 [01:05<12:31, 16.34s/it]\u001b[A\n",
            " 10%|█         | 5/50 [01:21<12:15, 16.35s/it]\u001b[A\n",
            " 12%|█▏        | 6/50 [01:38<12:00, 16.38s/it]\u001b[A\n",
            " 14%|█▍        | 7/50 [01:54<11:45, 16.42s/it]\u001b[A\n",
            " 16%|█▌        | 8/50 [02:11<11:29, 16.42s/it]\u001b[A\n",
            " 18%|█▊        | 9/50 [02:27<11:13, 16.42s/it]\u001b[A\n",
            " 20%|██        | 10/50 [02:44<10:59, 16.49s/it]\u001b[A\n",
            " 22%|██▏       | 11/50 [03:01<10:47, 16.61s/it]\u001b[A\n",
            " 24%|██▍       | 12/50 [03:17<10:31, 16.62s/it]\u001b[A\n",
            " 26%|██▌       | 13/50 [03:34<10:12, 16.56s/it]\u001b[A\n",
            " 28%|██▊       | 14/50 [03:50<09:54, 16.52s/it]\u001b[A\n",
            " 30%|███       | 15/50 [04:07<09:38, 16.53s/it]\u001b[A\n",
            " 32%|███▏      | 16/50 [04:23<09:24, 16.62s/it]\u001b[A\n",
            " 34%|███▍      | 17/50 [04:40<09:09, 16.66s/it]\u001b[A\n",
            " 36%|███▌      | 18/50 [04:57<08:55, 16.73s/it]\u001b[A\n",
            " 38%|███▊      | 19/50 [05:14<08:40, 16.78s/it]\u001b[A\n",
            " 40%|████      | 20/50 [05:31<08:23, 16.77s/it]\u001b[A\n",
            " 42%|████▏     | 21/50 [05:48<08:06, 16.79s/it]\u001b[A\n",
            " 44%|████▍     | 22/50 [06:05<07:52, 16.87s/it]\u001b[A\n",
            " 46%|████▌     | 23/50 [06:22<07:35, 16.88s/it]\u001b[A\n",
            " 48%|████▊     | 24/50 [06:38<07:17, 16.83s/it]\u001b[A\n",
            " 50%|█████     | 25/50 [06:55<07:01, 16.87s/it]\u001b[A\n",
            " 52%|█████▏    | 26/50 [07:12<06:45, 16.89s/it]\u001b[A\n",
            " 54%|█████▍    | 27/50 [07:29<06:28, 16.90s/it]\u001b[A\n",
            " 56%|█████▌    | 28/50 [07:46<06:11, 16.87s/it]\u001b[A\n",
            " 58%|█████▊    | 29/50 [08:03<05:55, 16.92s/it]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdc0CAMORmiA"
      },
      "source": [
        "{k:v for k, v in zip(search_result.space.dimension_names, search_result.x)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJKbA_-cRmiD"
      },
      "source": [
        "## Visualize search results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TBsBM7IRmiD"
      },
      "source": [
        "### Convergence of minima"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHQ82Go3RmiE"
      },
      "source": [
        "import skopt.plots\n",
        "fig = skopt.plots.plot_convergence(search_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T5Q9-MqRmiH"
      },
      "source": [
        "### Evaluation points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7srBlIGRmiI"
      },
      "source": [
        "fig = skopt.plots.plot_evaluations(result=search_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-1doTImRmiK"
      },
      "source": [
        "### Partial dependence plots (PDPs) of the estimated loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7RXPt2oRmiL"
      },
      "source": [
        "fig = skopt.plots.plot_objective(result=search_result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}